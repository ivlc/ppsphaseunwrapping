%\documentclass[draftcls, onecolumn, 11pt]{IEEEtran}
\documentclass[journal]{IEEEtran}

\usepackage{mathbf-abbrevs}

%\newcommand{\dealias}{\operatorname{dealias}}

\input{defs}

%\newcommand{\term}{\emph}
%\renewcommand{\index}[1]{}


\title{Polynomial phase estimation by phase unwrapping 1: Identifiability and strong consistency}
%\title{Polynomial phase estimation by phase unwrapping}

\author{Robby~G.~McKilliam, Barry~G.~Quinn, I.~Vaughan~L.~Clarkson, Bill~Moran and Badri~N.~Vellambi%
    \thanks{%A preliminary version of some of this material is contained in Part 3 of Robby McKilliam's PhD thesis \cite{McKilliam2010thesis}. 
Robby~McKilliam and Badri Vellambi are with the Institute for Telecommunications Research, The University of South Australia, SA, 5095.  Barry~Quinn is with the Department of Statistics, Macquarie University, Sydney, NSW, 2109, Australia.   Vaughan~Clarkson is with the School of Information Technology \& Electrical Engineering, The University of Queensland, QLD., 4072, Australia.  B. Moran is with the Department of Electrical Engineering and Computer
Science, Melbourne Systems Lab, Dept of Elec \& Electronic Eng, Uni of Melbourne, Vic. 3010, Australia.}}
% The paper headers 
\markboth{Polynomial phase estimation by phase unwrapping}{DRAFT \today}

% make the title area 

\begin{document} 
\maketitle

\begin{abstract}
Estimating the coefficients of a noisy polynomial phase signal is important in fields including radar, biology and radio communications. One approach attempts to perform polynomial regression on the phase of the signal.  This is complicated by the fact that the phase is \emph{wrapped} modulo $2\pi$ and must be \emph{unwrapped} before regression can be performed. %A recent approach suggested by the authors is to perform the unwrapping in a least squares manner.  %It was shown how the resulting estimator could be posed as a \emph{nearest lattice point problem} in a specific lattice and could be solved using existing algorithms. 
%It was shown by Monte Carlo simulation that this produces a remarkably accurate estimator.  
In this two part series of papers we consider an estimator that performs phase unwrapping in a least squares manner.  In this first part we describe conditions for the identifiability of polynomial phase signals and we prove the strong consistency of our unwrapping estimator.
%We describe the asymptotic properties of this estimator, showing that it is strongly consistent and asymptotically normally distributed. %We hypothesise that the estimator produces very near maximum likelihood performance.
\end{abstract}

\begin{keywords}
Polynomial phase signals, phase unwrapping, asymptotic properties, nearest lattice point problem
\end{keywords}
 
%\bibliographystyle{unsrt}

\section{Introduction} \label{intro}

Polynomial phase signals arise in fields including radar, sonar, geophysics, biology, and radio communication \cite{Angeby_estimating_2000}. In radar and sonar applications polynomial phase signals arise when acquiring radial velocity and acceleration (and higher order motion descriptors) of a target from a reflected signal, and also in continuous wave radar and low probability of intercept radar~\cite{Levanon_Radar_signals_2004}.  In biology, polynomial phase signals are used to describe the sounds emitted by bats and dolphins for echo location \citep{Suga_1975_bats_echolocation, Moss_2005echolocation}.  

A polynomial phase signal of order $m$ is a function of the form
\[
s(t) = e^{2\pi j y(t)},
\]
where $j = \sqrt{-1}$, and $t$ is a real number, often representing time, and 
\[
y(t) = \tilde{\mu}_0 +\tilde{\mu}_1 t + \tilde{\mu}_2 t^2 + \dots \tilde{\mu}_m t^m
\]
is a polynomial of order $m$.  In practice the signal is typically sampled at discrete points in `time', $t$. In this paper we only consider uniform sampling, where the gap between consecutive samples is constant. In this case we can always consider the samples to be taken at some set of consecutive integers and our sampled polynomial phase signal looks like
\[
s_n = s(n) = e^{2\pi j y(n)},
\] 
where $n$ is an integer.  Of practical importance is the estimation of the coefficients $\tilde{\mu}_0, \dots, \tilde{\mu}_m$ from a number, say $N$, of observations of the noisy sampled signal
\begin{equation}\label{eq:Y_nsamplednoisey}
Y_n = \rho s_n + X_n,
\end{equation}
where $\rho$ is a real number greater than zero representing the (usually unknown) signal amplitude and $\{X_n, n \in \ints\}$ is a sequence complex noise variables. In order to ensure identifiability it is necessary to restrict the $m+1$ coefficients to a region of $m+1$ dimensional Euclidean space $\reals^{m+1}$ called an \emph{identifiable region}.  It was shown in \cite{McKilliam2009IndentifiabliltyAliasingPolyphase} that an identifiable region tessellates a particular $m+1$ dimensional lattice.  We discuss this in Section \ref{sec:ident_aliasing}.

%A popular estimator is based on the \emph{discrete polynomial phase transform} (DPT) first introduced by Peleg and Porat \cite{Peleg_DPT_1995}.  The transform enables each parameter to be be estimated iteratively using the Fast Fourier transform in a way analogous to the frequency estimator of Rife and Boorstyn \cite{Rife1974, Quinn2001}.  The DPT estimator is characterized by good statistical performance and computational efficiency.  Variants of the DPT have been suggested by numerous authors including Golden and Friedlander \cite{Golden_mod_dpt_1998} and O'Shea \cite{Oshea_iterative_1996}. The recent \emph{high-order phase function} estimators are based on similar ideas \cite{Farquharson_another_poly_est_2005}.  A substantial drawback of all of these estimators is that they only work correctly over a small set of parameters, generally much smaller than the identifiable region.  This property appears to have been first noticed in \cite{McKilliam2009IndentifiabliltyAliasingPolyphase, McKilliam2009asilomar_polyest_lattice} and has been analysed further in \cite[Ch. 10]{McKilliam2010thesis}.
%
%Another estimator, described by Kitchen \cite{Kitchen_polyphase_unwrapping_1994}, is based on the phase unwrapping approach to frequency estimation suggested by Kay \cite{Kay1989}.  The primary advantage of Kitchen's estimator is that it requires only $O(N)$ arithmetic operations to compute. The primary disadvantage is that it only works correctly when the signal to noise ratio (SNR) is high. It was also observed in \cite{McKilliam2009asilomar_polyest_lattice} that Kitchen's estimator does not work correctly for all coefficients in the identifiable region, but, the problem is far less severe than for estimators based on the DPT \cite[Sec. 10.3]{McKilliam2010thesis}.

%An obvious estimator of the unknown coefficients is the least squares estimator (LSE).  This is also the maximum likelihood estimator (MLE) when the noise sequence $\{X_n\}$ is white and Gaussian.  When $m=0$ (phase estimation) or $m=1$ (frequency estimation) the LSE is an effective approach, being both computationally efficient and statistically accurate \cite{Quinn2009_dasp_phase_only_information_loss,Hannan1973,Quinn2001,McKilliam_mean_dir_est_sq_arc_length2010}\cite[Sec.~6.4~and~9.1]{McKilliam2010thesis}. When $m \geq 2$ the computational complexity of the LSE is large~\cite[Sec.~10.1]{McKilliam2010thesis}\cite{Abatzoglou_ml_chirp_1986}. For this reason many authors have considered alternative approaches to polynomial phase estimation. These can loosely be grouped into two classes, estimators based on polynomial phase transforms, such as the discrete polynomial phase transform (DPT)~\cite{Peleg_DPT_1995,Peleg1991_est_class_PPS_1991,Porat_asympt_HAF_DPT_1996} and the high order phase function~\cite{Farquharson_another_poly_est_2005}, and estimators based on phase unwrapping, such as Kitchen's unwrapping estimator~\cite{Kitchen_polyphase_unwrapping_1994}, the estimator of Djuric and Kay~\cite{Djuric_phase_unwrap_chirp_1990}, and Morelande's Bayesian unwrapping estimator~\cite{Morelande_bayes_unwrapping_2009_tsp}.

One estimation approach attempts to perform polynomial regression on the phase of the signal~\cite{Slocumb_polynomial_1994,Tretter1985,Kitchen_polyphase_unwrapping_1994,Djuric_phase_unwrap_chirp_1990,Morelande_bayes_unwrapping_2009_tsp}.  This is complicated by the fact that the phase is \emph{wrapped} modulo $2\pi$ and must be \emph{unwrapped} before regression can be performed.  In this two part series of papers we consider the estimator that results from unwrapping the phase in a least squares manner.  We call this the \emph{least squares unwrapping estimator} (LSU)~\cite{McKilliam2009asilomar_polyest_lattice, McKilliamFrequencyEstimationByPhaseUnwrapping2009}\cite[Chap. 8]{McKilliam2010thesis}.  It was shown in \cite{McKilliam2009asilomar_polyest_lattice, McKilliamFrequencyEstimationByPhaseUnwrapping2009} that the LSU estimator can be represented as a \emph{nearest lattice point problem}, and Monte-Carlo simulations were used to show the LSU estimator's favourable statistical performance. %Moreover, it was noticed that the LSU appears to work correctly for coefficients anywhere in the identifiable region.  A drawback of the LSU estimator is that computing a nearest lattice point is, in general, computationally difficult.  In \cite{McKilliam2009asilomar_polyest_lattice}, two standard techniques were considered, the \emph{sphere decoder} \cite{Pohst_sphere_decoder_1981}, and \emph{Babai's nearest plane algorithm} \cite{Babai1986}. The sphere decoder was observed to have excellent statistical performance but can only be computed efficiently for $N<50$ \cite{Jalden2005_sphere_decoding_complexity}.  Computing the Babai point requires only $O(N^2)$ operations, but its statistical performance is comparatively poor at low signal to noise ratio (SNR). A major point of interest is that the lattices considered are not \emph{random}, and therefore may admit fast nearest point algorithms. 
%This has been studied in~\cite[Sec.~4.2]{McKilliam2010thesis} where polynomial time algorithms where found that compute the nearest point exactly.  Unfortantely, although polynomial time, the algorithm are still computationally very slow in practice. 
% Two main questions were raised in \cite{McKilliam2009asilomar_polyest_lattice}:
% \begin{enumerate}
% \vspace{-0.1cm}
% \item What are the asymptotic statistical properties of the LSU estimator?
% \vspace{-0.18cm}
% \item Can the structure of the lattice be exploited to obtain fast (exact or approximate) nearest point algorithms and thereby a computationally efficient estimator?
% \vspace{-0.1cm}
% \end{enumerate}
% The purpose of this paper is to answer the first question. We prove that the LSU estimator is strongly consistent and asymptotically normally distributed. 
%We also apply another known approximate nearest point algorithm, called the $K$-best method \cite{Zhan2006_K_best_sphere_decoder}, that we show provides near sphere decoder performance, but can be computed in a reasonable amount of time if $N$ is less than about 300.
In this two part series of papers we derive the asymptotic statistical properties of the LSU estimator, showing that it is strongly consistent and asymptotically normally distributed.  Strong consistency is proved in this paper, while asymptotic normality is proved in the second part~\cite{McKilliam_pps2_2012}.  Similar results were stated without a complete proof in~\cite{McKilliam_polyphase_est_icassp_2011}.  Here, we give a proof.  The results here are also more general than in~\cite{McKilliam_polyphase_est_icassp_2011}, allowing for a wider class of noise distributions.

An interesting property is that the estimator of the $k$th polynomial phase coefficient converges to $\tilde{\mu}_k$ at rate $o(N^{-k})$.  This is perhaps not surprising, since it is the same rate observed in polynomial regression.  However, asserting that convergence at this rate occurs in the polynomial phase setting is not trivial.  For this purpose we make use of an elementary result about the number of arithmetic progressions contained inside subsets of $\{1,2,\dots,N\}$~\cite{Erdos_on_some_sequence_of_integers1936,Szemeredi_setint_no_k_arth1975,Gowers_new_proof2001}.  %This proof technique appears to be novel, and may be useful in other problems.  
%The proof of asymptotic normality is complicated by the fact that the objective function corresponding with the LSU estimator is not differentiable everywhere.  Empirical process techniques~\cite{Pollard_new_ways_clts_1986,Pollard_asymp_empi_proc_1989,van2009empirical,Dudley_unif_central_lim_th_1999} and results from the literature on hyperplane arrangements~\cite{Chazelle_discrepency_method_2000,Matousek_lect_disc_geom_2002} become useful here.  
We are hopeful that the proof techniques developed here will be useful for purposes other than polynomial phase estimation, and in particular other applications involving data that is `wrapped' in some sense.  Potential candidates are the phase wrapped images observed in modern radar and medical imaging devices such as synthetic aperture radar and magnetic resonance imaging~\cite{Nico_phaseunwrappingSAR_2000,Friedlander_PD_phaseunwrapping_1996}.

The paper is organised in the following way. Section~\ref{sec:lattice-theory} describes some required concepts from lattice theory.  In Section~\ref{sec:ident_aliasing} we describe the identifiable region that was also derived in~\cite{McKilliam2009IndentifiabliltyAliasingPolyphase}.  These identifiability results are required in order to properly understand the statistical properties of polynomial phase estimators.  Section~\ref{sec:least-squar-unwr} describes the LSU estimator and states a theorem asserting the estimator to be strongly consistent. The theorem is proved in Section~\ref{sec:strongconstproof}.  In the second paper~\cite{McKilliam_pps2_2012} we prove the asymptotic normality of the LSU estimator and describe the results of Monte Carlo simulations.  These simulations agree with the derived asymptotic statistical properties. 

%\subsection{Notation}
%We write random variables using capital letters, such as $X$ and $Y$ and circular random variables using the capital Greek letters $\Theta$ and $\Phi$.  We use $\round{x}$ to denote the nearest integer to $x$ with half integers rounded up and use $\fracpart{x} = x - \round{x}$ to denote the \emph{centred} fractional part.  Both $\round{\cdot}$ and $\fractpart{\cdot}$ operate elementwise on vectors.


\section{Lattice Theory}\label{sec:lattice-theory}

A \term{lattice},  $\Lambda$, is a discrete subset of points in $\reals^n$ such that
\[
   \Lambda = \{\xbf = \Bbf\ubf \mid \ubf \in \ints^d \}
\]
where $\Bbf \in \reals^{n \times d}$ is an $n \times d$ matrix of rank $d$, called the generator matrix.  If $n = d$ the lattice is said to be full rank.  Lattices are discrete Abelian groups under vector addition.  They are subgroups of the Euclidean group $\reals^n$.  Lattices naturally give rise to tessellations of $\reals^n$ by the specification of a set of coset representatives for the quotient $\reals^n / \Lambda$.  One choice for a set of coset representatives is a fundamental parallelepiped; the parallelepiped generated by the columns of a generator matrix.  Another choice is based on the Voronoi cell; those points from $\reals^n$ nearest (with respect to the Euclidean norm in this paper) to the lattice point at the origin.  It is always possible to construct a rectangular set of representatives, as the next proposition will show.  We will use these rectangular regions for describing the aliasing properties of polynomial phase signals in Section~\ref{sec:ident_aliasing}.  These rectangular regions will be important for the derivation of the asymptotic properties of the LSU estimator.
% in Section~\ref{sec:least-squar-unwr}.

\begin{proposition}\label{prop:lattice-theory-constructing_a_rectangular_tesselating_region}
Let  $\Lambda$ be an $n$ dimensional lattice and $\Bbf \in \reals^{n\times n}$ be a generator matrix for $\Lambda$. Let $\Bbf = \Qbf\Rbf$ where $\Qbf$ is orthonormal and $\Rbf$ is upper triangular with elements $r_{ij}$.  Then the rectangular prism $\Qbf P$ where
\[
P = \prod_{k=1}^{n}{\left[-\frac{r_{kk}}{2}, \frac{r_{kk}}{2}\right)}
\]
is a set of coset representatives for $\reals^n / \Lambda$.
\end{proposition}
\begin{IEEEproof}
This result is well known~\cite[Chapter IX, Theorem IV]{Cassels_geom_numbers_1997}~\cite[Proposition 2.1]{McKilliam2010thesis}.  This result is for lattices with full rank.  A result in the general case can be obtained similarly, but is not required here.  
\end{IEEEproof}

\begin{figure}[tp]
	\centering
		\includegraphics[width=\linewidth]{plots/tesselationfigures-2.mps}
		\caption{Rectangular tessellation constructed according to Proposition~\ref{prop:lattice-theory-constructing_a_rectangular_tesselating_region} where $\Lambda$ is a 2 dimensional lattice with generator matrix having columns $[1, 0.2]^\prime$ and $[0.2, 1]^\prime$. Any one of the boxes is a rectangular set of coset representatives for $\reals^2 / \Lambda$.  The shaded box centered at the origin is the one given by Proposition~\ref{prop:lattice-theory-constructing_a_rectangular_tesselating_region}.}
		\label{lattices:fig:tessellation2}
\end{figure} 

\section{Identifiability and aliasing}\label{sec:ident_aliasing}

As discussed in the introduction, a polynomial phase signal of order $m$ is a complex valued function of the form $s(t) = e^{2\pi j y(t)}$ where $t$ is a real number and $y(t)$ is a polynomial of order $m$. We will often drop the $(t)$ and just write the polynomial as $y$ and the polynomial phase signal as $s$ whenever there is no chance of ambiguity. %In practice the signal obtained is typically \emph{sampled} at discrete points in time, $t$. In this thesis we only consider \term{uniform sampling}\index{uniform sampling}, that is, where the gap between consecutive samples is a constant. In this case we can always consider the samples to be taken at some set of consecutive integers and our sampled polynomial phase signal looks like
%\[
%s(n) = e^{2\pi j y(n)}
%\] 
%where $n$ is an integer. The phase of $s(n)$ is described by the sampled polynomial
%\[
%y(n) = \mu_0 + \mu_1 n + \mu_2 n^2 + \dots + \mu_m n^m.
%\]
Aliasing can occur when polynomial-phase signals are sampled.  That is, two or more distinct polynomial-phase signals can take exactly the same values at the sample points.  These aliasing results are also given in~\cite{McKilliam2009IndentifiabliltyAliasingPolyphase}, but the presentation here is different, and is better suited to studying the asymptotic properties of the LSU estimator.  %A description of the aliasing properties of polynomial phase signals of order $2$ has been independently discovered in~\cite{Abatzoglou_ml_chirp_1986}~and~\cite{Angeby_PPS_aliasing_2000}.  The results here are for polynomial phase signals of any order.

Let $\mathcal{Z}$ be the set of polynomials of order at most $m$ that take integer values when evaluated at integers. That is, $\mathcal{Z}$ contains all polynomials $p$ such that $p(n)$ is an integer whenever $n$ is an integer.
Let $y$ and $z$ be two \emph{distinct} polynomials such that $z = y + p$ for some polynomial $p$ in $\mathcal{Z}$. The two polynomial phase signals
\[
s(t) = e^{2\pi j y(t)} \qquad \text{and} \qquad r(t) = e^{2\pi j z(t)}
\]
are distinct because $y$ and $z$ are distinct, but if we sample $s$ and $r$ at the integers  
\begin{align*}
s(n) &= e^{2\pi j y(n)} =  e^{2\pi j y(n)} e^{2\pi j p(n)} \\
&= e^{2\pi j (y(n) + p(n))} = e^{2\pi j z(n)} = r(n)
\end{align*}
because $p(n)$ is always an integer and therefore $e^{2\pi j p(n)} = 1$ for all $n \in \ints$. The polynomial phase signals $s$ and $r$ are equal at the integers, and although they are distinct, they are indistinguishable from their samples. We call such polynomial phase signals \term{aliases}\index{alias} and immediately obtain the following theorem.

\begin{theorem}\label{thm:circpolysampledthm}
Two polynomial phase signals $s(t) = e^{2\pi j y(t)}$  and $r(t) = e^{2\pi j z(t)}$  are aliases if and only if the polynomials that define their phase, $y$ and $z$, differ by a polynomial from the set $\mathcal{Z}$, that is, $y - z \in \mathcal{Z}$.
\end{theorem}

%\begin{corollary}\label{cor:circpolysampledexp}
%The polynomial phase signals $s(t)$ and $r(t)$ are aliases if and only if $s(t) = r(t)e^{2\pi j p(t)}$ where $p$ is a polynomial from $\mathcal{Z}$.
%\end{corollary}

%It may be helpful to observe Figures~\ref{fig:circstatplot_zero},~\ref{fig:circstatplot_line},~\ref{fig:circstatplot_quad} and~\ref{fig:circstatplot_cube}. In these, the phase (divided by $2\pi$) of two distinct polynomial phase signals is plotted on the left, and on the right the principal component of the phase is plotted. The circles display the samples at the integers. Note that the samples of the principal components intersect.  The corresponding polynomial phase signals are aliases.

It may be helpful to observe Figures~\ref{fig:circstatplot_zero} to~\ref{fig:circstatplot_cube}.  In these, the phase (divided by $2\pi$) of two distinct polynomial phase signals is plotted on the left, and on the right the principal component of the phase (also divided by $2\pi$) is plotted. The circles display the samples at the integers. Note that the samples of the principal components intersect.  The corresponding polynomial phase signals are aliases.

We can derive an analogue of the theorem above in terms of the coefficients of the polynomials $y$ and $z$. This will be useful when we consider estimating the coefficients in Section~\ref{sec:least-squar-unwr}.  We first need the following family of polynomials. 

\begin{definition} \emph{(Integer valued polynomials)} \label{def:intvaledpolys}
\\The integer valued polynomial of order $k$, denoted by $p_k$, is
\[
p_k(x) = \binom{x}{k} = \frac{x(x-1)(x-2)\dots(x-k+1)}{k!},
\]
where we define $p_0(x) = 1$.
\end{definition}

\begin{lemma}\label{lem:intvalpol}
  The integer valued polynomials $p_0,\dots,p_m$ are an integer basis for $\mathcal{Z}$.  That is, every polynomial in $\mathcal{Z}$ can be uniquely written as
\begin{equation} \label{eq:lem_polynomial}
c_0 p_0 + c_1 p_1 + \dots + c_m p_m,
\end{equation}
where the $c_i \in \ints$.
\end{lemma}
\begin{IEEEproof}
See~\citep[p. 2]{cahen_integer-valued_1997} or~\cite{McKilliam2009IndentifiabliltyAliasingPolyphase}. 
\end{IEEEproof}
% \begin{IEEEproof}
% Note that $x(x-1)(x-2)\dots(x-k+1)$ is divisible by all integers $1,2,\dots,k$ and so $p_k$ takes integer values for all $x\in\ints$.  Then any polynomial generated as in \eqref{eq:lem_polynomial} is an element in $\mathcal{Z}$.  The proof proceeds by induction.  Consider any polynomial $f \in \mathcal{Z}$.  Let $d < n$ and assume that $c_i \in \ints$ for all $i \leq d$.  Let $g$ be the polynomial
% \[
% g = f - \sum_{k=0}^{d}{c_k p_k}
% \]
% and note that $g \in \mathcal{Z}$. Then
% \begin{equation}
% g = c_{d+1}p_{d+1} + \dots + c_{m}p_{m}.
% \end{equation}
% Now $p_{d+1}(d+1) = 1$ and $p_k(d+1) = 0$ for all $k>d+1$.  Then $g(d+1) = c_{d+1}p_{d+1}(d+1)$ and therefore $c_{d+1} = g(d+1) \in \ints$.  The proof follows by induction because $f(0) = c_0 \in \ints$.
% \end{IEEEproof}

Given a polynomial $g(x) = a_0 + a_1x + \dots + a_m x^m$, let
\[
\coef(g) = \left[ \begin{array}{ccccc} a_0 & a_1 & a_2 & \dots & a_m \end{array} \right]^\prime,
\]
where superscript $^\prime$ indicates the transpose, denote the column vector of length $m+1$ containing the coefficients of $g$.  If $y$ and $z$ differ by a polynomial from $\mathcal{Z}$ then we can write $y = z + p$ where $p \in \mathcal{Z}$ and then also $\coef(y) = \coef(z) + \coef(p)$.
%\footnote{In group theory terminology $\coef(\cdot)$ coupled with vector addition is called a \term{group homomorphism}.\index{group homomorphism}}. 
  Consider the set
\[
L_{m+1} = \{ \coef(p) \mid p \in \mathcal{Z} \},
\]
containing the coefficient vectors corresponding to the polynomials in $\mathcal{Z}$.  Since the integer valued polynomials are a basis for $\mathcal{Z}$,
\begin{align*}
L_{m+1} &= \{ \coef(c_0 p_0 + c_1p_1 + \dots + c_mp_m) \mid c_i \in \ints \} \\
&= \{ c_0 \coef(p_0) + \dots + c_m\coef(p_m) \mid c_i \in \ints \}.
\end{align*}
Let
\[
\Pbf = \left[ \begin{array}{cccc} \coef(p_0)& \coef(p_1)& \dots& \coef(p_m)  \end{array} \right]
\]
be the $m+1$ by $m+1$ matrix with columns given by the coefficients of the integer valued polynomials.  Then,
\[
L_{m+1} = \{ \xbf = \Pbf\ubf \mid \ubf \in \ints^{m+1} \}
\]
and it is clear that $L_{m+1}$ is an $m+1$ dimensional lattice.  That is, the set of coefficients of the polynomials from $\mathcal{Z}$ forms a lattice with generator matrix $\Pbf$. We can restate Theorem~\ref{thm:circpolysampledthm} as:
\begin{corollary}\label{cor:circpolysampledcoef}
Two polynomial phase signals $s(t) = e^{2\pi j y(t)}$  and $r(t) = e^{2\pi j z(t)}$ are aliases if and only if $\coef(y)$ and $\coef(z)$ differ by a lattice point in $L_{m+1}$.
\end{corollary}

\begin{figure}[p]
	\centering
		\includegraphics[width=\linewidth]{plots/circstatfigzero-1.mps}
		\caption{The zeroth order polynomials $\tfrac{7}{10}$ (solid line) and $\tfrac{17}{10}$ (dashed line).}		
\label{fig:circstatplot_zero}
\end{figure}

\begin{figure}[p]
	\centering
		\includegraphics[width=\linewidth]{plots/circstatfig-1.mps}
		\caption{The first order polynomials $\tfrac{1}{10}(3 + 8t)$ (solid) and $\tfrac{1}{10}(33 - 2t)$ (dashed line).}
		\label{fig:circstatplot_line}
\end{figure}

\begin{figure}[p]
	\centering
		\includegraphics[width=\linewidth]{plots/circstatfigquad-1.mps}
		\caption{The quadratic polynomials $\tfrac{1}{10} (15 - 15 t + 4 t^2)$ (solid line) and $\tfrac{1}{10}(25 -  t^2)$  (dashed line).}
		\label{fig:circstatplot_quad}
\end{figure}

\begin{figure}[p]
	\centering
		\includegraphics[width=\linewidth]{plots/circstatfigcube-1.mps}
		\caption{The cubic polynomials $\tfrac{1}{160} (174 + 85 t - 118 t^2 + 40 t^3)$ (solid line) and $\tfrac{1}{48} (84 + 19 t + 12 t^2 - 4 t^3)$  (dashed line).}
		\label{fig:circstatplot_cube}
\end{figure}

For the purpose of estimating the coefficients of a polynomial phase signal we must (in order to ensure identifiability) restrict the set of allowable coefficients so that no two polynomial phase signals are aliases of each other. In consideration of Corollary~\ref{cor:circpolysampledcoef} we require that the coefficients of $y(t)$, written in vector form $\mubf$, are contained in a set of coset representatives for the quotient $\reals^{m+1}/L_{m+1}$.  We call the chosen set of representatives the \term{identifiable region}\index{identifiable region}.

As an example consider the polynomial phase signal of order zero $e^{2\pi j \mu_0}$.  Since $e^{2\pi j \mu_0} = e^{2\pi j(\mu_0 + k)}$ for any integer $k$ we must, in order to ensure identifiability, restrict $\mu_0$ to some interval of length $1$.  A natural choice is the interval $[-\nicefrac{1}{2}, \nicefrac{1}{2})$. The lattice $L_1$ is the 1-dimensional integer lattice $\ints$ and the interval $[-\nicefrac{1}{2}, \nicefrac{1}{2})$ corresponds to the Voronoi cell of $L_1$. 
When $m=1$ it turns out that a natural choice of identifiable region is the square box $[-\nicefrac{1}{2}, \nicefrac{1}{2})^2$. This corresponds with the \term{Nyquist criterion}.  The lattice $L_2$ is equal to $\ints^2$ so the box $[-\nicefrac{1}{2}, \nicefrac{1}{2})^2$ corresponds with the Voronoi cell of $L_2$.  
When $m > 1$ the identifiable region becomes more complicated and $L_{m+1} \neq \ints^{m+1}$. %However, we can always construct an identifiable region by taking a tessellating region of lattice $L_{m+1}$.  %Two convenient tessellating regions are the Voronoi cell of $L$ and the rectangular tessellating region given by~\eqref{eq:rectangular_identifiable_region}.

In general there are infinitely many choices for the identifiable region. A natural choice is the Voronoi cell of $L_{m+1}$ used in~\cite{McKilliam2009IndentifiabliltyAliasingPolyphase}. Another potential choice is a fundamental parallelepiped of $L_{m+1}$. In this paper we will use the rectangular set constructed using Proposition~\ref{prop:lattice-theory-constructing_a_rectangular_tesselating_region}. Observe that $\Pbf$ is upper triangular with $k$th diagonal element equal to $\tfrac{1}{k!}$.  So this rectangular region is
\begin{equation}\label{eq:rectangular_identifiable_region}
B = \prod_{k=0}^{m}\left[ -\frac{0.5}{k!}, \frac{0.5}{k!}  \right).
\end{equation}
We will make use of this region when deriving the statistical properties of the LSU estimator in the next section. 

Given vectors $\xbf$ and $\ybf$ in $\reals^{m+1}$ we say that $\xbf \equiv \ybf \bmod L_{m+1}$ if $\xbf$ and $\ybf$ differ by a lattice point in $L_{m+1}$.  We define the function $\dealias(\xbf)$ to take $\xbf$ to its coset representative inside $B$. That is, $\dealias(\xbf) = \zbf \in B$ where $\xbf - \zbf \in L_{m+1}$.  %The effect of $\dealias(\xbf)$ is to \emph{dealias} the polynomial phase coefficients. 
When $m = 0$ or $1$ then $\dealias(\xbf) = \fracpart{\xbf}$ where $\fracpart{\xbf} = \xbf - \round{\xbf}$ denotes the (centered) fractional part and $\round{\xbf}$ denotes the nearest integer to $\xbf$ with half integers rounded upwards and both $\fracpart{\cdot}$ and $\round{\cdot}$ operate on vectors elementwise.  For $m \geq 2$ the function $\dealias(\xbf)$ can be computed by a simple sequential algorithm \cite[Sec. 7.2.1]{McKilliam2010thesis}.

\section{The least squares unwrapping estimator}\label{sec:least-squar-unwr}

We now describe the least squares unwrapping (LSU) estimator of the polynomial coefficients. Recall that we desire to estimate the coefficients $\tilde{\mu}_0, \dots, \tilde{\mu}_m$ from the noisy samples $Y_1, \dots, Y_N$ given in~\eqref{eq:Y_nsamplednoisey}.  We take the complex argument of the $Y_n$ and divide by $2\pi$ to obtain
\begin{equation}\label{eq:noise_circ_poly}
\Theta_n = \frac{\angle{Y_n}}{2\pi} = \fracpart{ \Phi_n + y(n) }
\end{equation}
where $\angle{z}$ denotes the complex argument of the complex number $z$, and 
\[
\Phi_n = \frac{1}{2\pi}\angle(1 + \rho^{-1}s_n^{-1}X_n)
\] 
are random variables representing the phase noise induced by the $X_n$~\cite{Tretter1985,Quinn2009_dasp_phase_only_information_loss}.  If the distribution of $X_n$ is circularly symmetric (i.e., the angle $\angle X_n$ is uniformly distributed on $[-\pi, \pi)$ and is independent of the magnitude $\abs{X_n}$) then the distribution of $\Phi_n$ is the same as the distribution of $\tfrac{1}{2\pi}\angle(1 + \rho^{-1}X_n)$.  If the $X_1, \dots, X_N$ are circularly symmetric and identically distributed, then $\Phi_1, \dots, \Phi_n$ are also identically distributed.

Let $\mubf$ be the vector $[\mu_0, \mu_1, \dots, \mu_m]$ and put,
\begin{equation} \label{eq:sumofsquaresfunction}
SS(\mubf) = \sum_{n=1}^{N}\fracpart{  \Theta_{n} - \sum_{k = 0}^{m}{\mu_k n^k} }^{2}.
\end{equation}
The least squares unwrapping estimator is defined as those coefficients $\widehat{\mu}_0, \dots, \widehat{\mu}_m$ that minimise $SS$ over the identifiable region $B$. That is, the LSU estimator is,
\begin{equation}\label{eq:hatmubfLSUdefn}
\widehat{\mubf} = \arg \min_{\mubf \in B} SS(\mubf). 
\end{equation}

It is shown in~\cite[Sec~8.1]{McKilliam2010thesis}\cite{McKilliam2009asilomar_polyest_lattice} how this minimisation problem can be posed as that of computing a nearest lattice point in a particular lattice. Polynomial time algorithms that compute the nearest point are described in~\cite[Sec.~4.3]{McKilliam2010thesis}.  Although polynomial in complexity, these algorithms are not fast in practice.  The existence of practically fast nearest point algorithms for these lattices is an interesting open problem.  In this paper we focus on the asymptotic statistical properties of the LSU estimator, rather than computational aspects.

The next theorem describes the asymptotic properties of this estimator.  Before we give the proof it is necessary to understand some of the properties of the phase noise $\Phi_1,\dots,\Phi_N$, which are \emph{circular} random variables with support on $[-\nicefrac{1}{2}, \nicefrac{1}{2})$~\cite{McKilliam2010thesis,McKilliam_mean_dir_est_sq_arc_length2010,Mardia_directional_statistics,Fisher1993}.  Circular random variables are often considered modulo $2\pi$ and therefore have support $[-\pi, \pi)$ with $-\pi$ and $\pi$ being identified as equivalent.  Here we instead consider circular random variables modulo 1 with support $[-\nicefrac{1}{2}, \nicefrac{1}{2})$ and with $-\nicefrac{1}{2}$ and $\nicefrac{1}{2}$ being equivalent.  This is nonstandard but it allows us to use notation such as $\round{\cdot}$ for rounding and $\fracpart{\cdot}$ for the centered fractional part in a convenient way.   %The random variable $2\pi \Phi_n$ has support on $[-\pi, \pi)$ and can be identified as a \emph{circular random variable}~\cite{Mardia_directional_statistics,Fisher1993}.  %Let $f$ be the probability density function (pdf) of $\Phi_n$.  Then $f$ takes nonzero values only on $[-\nicefrac{1}{2}, \nicefrac{1}{2})$.  It is instructive to think of $f$ as describing a describing a distribution on a circle.  For example, when the noise term $X_n$ is complex Gaussian with independent and identically distributed real and imaginary parts, the distribution of $\Phi_n$ is the projected Guassian~\cite[Section 5.6.1]{McKilliam2010thesis}\cite[page 46]{Mardia_directional_statistics}.  It is instrutive the consider a periodic version of $f$, that we write as $f(\fracpart{x})$, which has period equal to one, because the fractional part function $\fracpart{x}$ has period one. 

The \emph{intrinsic mean} or \emph{Fr\'{e}chet mean} of $\Phi_n$ is defined as~\cite{McKilliam_mean_dir_est_sq_arc_length2010,bwhk07a,Bhattacharya_int_ext_means_2003,Bhattacharya_int_ext_means_2005},
\begin{equation}\label{eq:intrmeandefn}
 \mu_{\text{intr}}  = \arg \min_{\mu \in [-\nicefrac{1}{2}, \nicefrac{1}{2})} \expect \fracpart{\Phi_n - \mu}^2, 
% %&= \arg \min_{\mu \in [-\nicefrac{1}{2}, \nicefrac{1}{2})}\int_{-\nicefrac{1}{2}}^{\nicefrac{1}{2}}\fracpart{\phi - \mu}^2f(\theta) d\theta. \label{eq:minunrappedmeandef}
\end{equation}
and the \emph{intrinsic variance} is
\[
\sigma_{\text{intr}}^2 = E\fracpart{\Theta - \mu_{\text{intr}}}^2 = \min_{\mu \in [-\nicefrac{1}{2}, \nicefrac{1}{2})} \expect \fracpart{\Phi_n - \mu}^2,
\]
where $\expect$ denotes the expected value.  Depending on the distribution of $\Phi_n$ the argument that minimises~\eqref{eq:intrmeandefn} may not be unique.  The set of minima is often called the~\emph{Fr\'{e}chet mean set}~\cite{Bhattacharya_int_ext_means_2003,Bhattacharya_int_ext_means_2005}.  If the minimiser is not unique we say that $\Phi_n$ has no intrinsic mean.  %We are now equipped to state the asymptotic properties of the LSU estimator.
We are now equipped to state the main result of this paper.

%\begin{figure}[tp]
% 	\centering 
% 		\includegraphics[width=\linewidth]{plots/distplots.class.distributions.circular.ProjectedNormalDistribution.var3.0-1.mps}
% 		\caption{The projected normal distribution.}
% 		\label{fig:circularuniformdist}
%\end{figure}

%BLERG NOTE: theorem should be rewritten to instead assume that the X_1, \dots X_N are circularly symmetric.  Then the continuity stuff for the f follows immediately.  You can then talk about how it could apply more generally, but gets complicated.
 
\begin{theorem} \label{thm:asymp_proof}  (Strong consistency) 
Let $\widehat{\mubf}$ be defined by~\eqref{eq:hatmubfLSUdefn} and put $\widehat{\lambdabf}_N = \dealias(\tilde{\mubf} - \widehat{\mubf})$.  Denote the elements of $\widehat{\lambdabf}_N$ by $\widehat{\lambda}_{0,N}, \dots, \widehat{\lambda}_{m,N}$.  Suppose $\Phi_1, \dots, \Phi_N$ are independent and identically distributed with zero intrinsic mean and intrinsic variance $\sigma^2$, then $N^k \widehat{\lambda}_{k,N}$ converges almost surely to $0$ as $N\rightarrow\infty$ for all $k = 0, 1, \dots, m$.
\end{theorem}

A proof of this theorem is given in Section~\ref{sec:strongconstproof}.  A proofs for the case when $m=0$ was given in~\cite{McKilliam_mean_dir_est_sq_arc_length2010} and for the case when $m=1$ was given in~\cite{McKilliamFrequencyEstimationByPhaseUnwrapping2009}.  The proof here takes a similar approach, but requires new techniques.  %Before giving the proof we make the following remarks.
The theorem gives conditions on the \emph{dealiased} difference $\dealias(\tilde{\mubf} - \widehat{\mubf})$ between the true coefficients $\tilde{\mubf}$ and the estimated coefficients $\widehat{\mubf}$ rather than directly on the difference $\tilde{\mubf} - \widehat{\mubf}$.   To see why this makes sense, consider the case when $m=0$, $\tilde{\mu}_0 = -0.5$ and $\widehat{\mu}_0 = 0.49$, so that $\tilde{\mu}_0 - \widehat{\mu}_0 = -0.99$.  However, the two phases are obviously close, since the phase $\pm 0.5$ are actually the same.  In this case 
\[
\dealias(\tilde{\mu}_0 - \widehat{\mu}_0) = \fracpart{\tilde{\mu}_0 - \widehat{\mu}_0} = 0.01.
\] 
The same reasoning holds for $m > 0$.

The requirement that $\Phi_1, \dots, \Phi_N$ be identically distributed will typically hold only when the complex random variables $X_1, \dots, X_N$ are identically distributed and circularly symmetric.  It would be possible to drop the assumption that $\Phi_1, \dots, \Phi_N$ be identically distributed, but this complicates the theorem statement and the proof.  In the interest of simplicity we only consider the case when $\Phi_1, \dots, \Phi_N$ are identically distributed here.  If $X_n$ is circularly symmetric with density function nonincreasing with magnitude $\abs{X_n}$, then, the corresponding $\Phi_n$ necessarily has zero intrinsic mean~\cite[Theorem~5.2,~page~78]{McKilliam2010thesis}.  Thus, our theorem covers commonly used distributions for $X_1, \dots, X_N$, such as the normal distribution.

Although we will not prove it here the assumption that $\Phi_1,\dots,\Phi_N$ have zero intrinsic mean is not only sufficient, but also necessary, for if $\Phi_1,\dots,\Phi_N$ have intrinsic mean $x \in [-\nicefrac{1}{2},\nicefrac{1}{2})$ with $x \neq 0$ then $\fracpart{\widehat{\lambda}_{0,N} - x}\rightarrow 0$ almost surely as $N\rightarrow\infty$, and so $\widehat{\lambda}_{0,N}$ does not converge to zero.  On the other hand if $\Phi_1,\dots,\Phi_N$ do not have an intrinsic mean then $\widehat{\lambda}_{0,N}$ will not converge.

%The proof of asymptotic normality places requirements on the pdf $f$ of the phase noise.  The requirement that $\Phi_1, \dots, \Phi_N$ have zero intrinsic mean implies that $f(-\nicefrac{1}{2}) \leq 1$~\cite[Lemma~1]{McKilliam_mean_dir_est_sq_arc_length2010}, so the only case not handled is when equality holds, i.e., when $f(-\nicefrac{1}{2}) = 1$ or when $f(\fracpart{x})$ is discontinuous at $x = -\nicefrac{1}{2}$. In this exceptional case other expressions for the asymptotic variance can be found (similar to \cite[Theorem 3.1]{Hotz_circle_means_2011}), but this comes at a substantial increase in complexity and for this reason we have omitted them. %Distributions that do not satisfy these requirements are unlikely to be needed in practice.

%Finally note that $\Cbf$ is a \emph{Hilbert matrix}\index{Hilbert matrix} and the elements of the inverse $\Cbf^{-1}$ are given by
 %\[
 %C^{-1}_{ik} =\frac{i+k+1}{(-1)^{i+k}}{m+i+1 \choose m-k}{m+k+1 \choose m-i}{i+k \choose i}^2.
 %\]
%Hilbert matrices are particularly ill conditioned and difficult to numerically invert.  This problem can be avoided using the above formula. 
%I MIGHT HAVE AN ERROR IN THIS FORMULA!

\section{Proof of strong consistency}\label{sec:strongconstproof}
 Substituting \eqref{eq:noise_circ_poly} into $SS$ we obtain
 \begin{align*}
SS\left( \mubf \right) &= \sum_{n=1}^{N}\fracpart{ \fracpart{ \Phi_n + \sum_{k = 0}^{m}{\tilde{\mu}_k n^k} } - \sum_{k = 0}^{m}{\mu_k n^k} }^{2} \\
&= \sum_{n=1}^{N}\fracpart{  \Phi_n + \sum_{k = 0}^{m}{(\tilde{\mu}_k - \mu_k) n^k} }^{2}.
\end{align*}
Let $\lambdabf = \dealias(\tilde{\mubf} - \mubf) = \tilde{\mubf} - \mubf - \pbf$ where $\pbf$ is a lattice point from $L_{m+1}$. From the definition of $L_{m+1}$ we have $p_0 + p_1 n + \dots + p_{m} n^m$ an integer whenever $n$ is an integer, so
\begin{align*}
\fracpart{\sum_{k=0}^{m}\lambda_k n^k } &= \fracpart{\sum_{k=0}^{m}(\tilde{\mu}_k - \mu_k - p_k) n^k } \\
&= \fracpart{\sum_{k=0}^{m}(\tilde{\mu}_k - \mu_k) n^k }.
\end{align*}
Let
\[
SS\left( \mubf \right) = \sum_{n=1}^{N}\fracpart{  \Phi_n + \sum_{k = 0}^{m}{\lambda_k n^k} }  ^{2} = N S_{N}\left( \lambdabf \right).
 \]
From the definition of the $\dealias(\cdot)$ function $\lambdabf \in B$ so the elements of $\lambdabf$ satisfy
 \begin{equation} \label{eq:identifiability}
 -\frac{0.5}{k!} \leq \lambda_k < \frac{0.5}{k!}.
 \end{equation} 
Now $\widehat{\lambdabf}_N = \dealias(\tilde{\mubf} - \widehat{\mubf})$ is the minimiser of $S_{N}$ in $B$.  %We now analyse the minimiser of $S_N$.  
Let
\[
V_N(\lambdabf) =  \expect S_N(\lambdabf) = \frac{1}{N}\sum_{n=1}^{N} \expect \fracpart{  \Phi_{n}+\sum_{k = 0}^{m}{\lambda_k n^k}}  ^{2}.
\]
It will follow that
 \begin{equation}\label{eq:SNVNunifmlln}
\sup_{\lambdabf \in B}\sabs{S_N(\lambdabf) - V_N(\lambdabf)} \rightarrow 0  
 \end{equation}
almost surely as $N\rightarrow\infty$.  This type of result has been called a \emph{uniform law of large numbers} and follows from standard techniques~\cite{Pollard_conv_stat_proc_1984}.  We give a full proof of~\eqref{eq:SNVNunifmlln} in Appendix~\ref{app:uniform-law-large}.  We now concentrate attention on the minimiser of $V_N$. Because $\Phi_n$ has zero intrinsic mean 
\begin{equation}\label{eq:Efracpartmined}
\expect \fracpart{ \Phi_n + z }^{2}
\end{equation}
is minimised uniquely at $z = 0$ for $z \in [-\nicefrac{1}{2}, \nicefrac{1}{2})$.  Since the intrinsic variance of $\Phi_n$ is $\sigma^2$, when $z = 0$,
\begin{equation}\label{eq:Efracpartphi}
\expect\fracpart{\Phi_1+z}^{2} = \expect\fracpart{\Phi_1}^{2} = \sigma^2,
\end{equation}
and so the minimum attained value is $\sigma^2$.

\begin{lemma}\label{lem:ES_Nminimisedzero}
For $\lambdabf \in B$ the function $V_N(\lambdabf)$ is minimised uniquely at $\zerobf$, the vector of all zeros.  At this minimum $V_N(\zerobf) = \sigma^2$.
\end{lemma}
\begin{proof}
Put $z(n) = \lambda_0 + \lambda_1 n + \dots + \lambda_m n^m$.  Then
\begin{align*}
V_N(\lambdabf) &= \frac{1}{N}\expect\sum_{n=1}^{N}\fracpart{ \Phi_n + \sum_{k=0}^m{\lambda_k n^k} }^2 \\
&= \frac{1}{N}\sum_{n=1}^{N}\expect\fracpart{ \Phi_n + \fracpart{z(n)} }^2.
\end{align*}
We know that $\expect\fracpart{ \Phi_n + \fracpart{z(n)} }^2$ is minimised uniquely when $\fracpart{z(n)} = 0$ at which point it takes the value $\sigma^2$. Now $\fracpart{z(n)}$ is equal to zero for all integers $n$ if and only if $z \in \mathcal{Z}$, or equivalently if $\coef(z)$ is a lattice point in $L_{m+1}$. By definition $B$ contains precisely one lattice point from $L_{m+1}$, this being the origin $\zerobf$. Therefore $V_N$ is minimised uniquely at $\zerobf$, at which point it takes the value $\sigma^2$.
\end{proof}

\begin{lemma} \label{lem:ESNconv}
$\sabs{V_N(\widehat{\lambdabf}_N) - \sigma^2} \rightarrow 0$ almost surely as $N \rightarrow \infty$.
\end{lemma}
\begin{proof}
By definition $\widehat{\lambdabf}_N =\arg\min_{\lambdabf\in B} S_N(\lambdabf)$ so 
\[
0 \leq S_{N}(\zerobf) - S_N(\widehat{\lambdabf}_N).
\]  
Also, because $V_N$ is minimised at $\zerobf$, it follows that 
%\[
%0 \leq V_N(\widehat{\lambdabf}_N) - V_N(\zerobf).
%\]  
%Combining the two inequalities above,
\begin{align*}
0 &\leq V_N(\widehat{\lambdabf}_N) - V_N(\zerobf) \\
 &\leq V_N(\widehat{\lambdabf}_N) - V_N(\zerobf) + S_{N}(\zerobf) - S_N(\widehat{\lambdabf}_N)   \\
&\leq \sabs{ V_N(\widehat{\lambdabf}_N) - S_N(\widehat{\lambdabf}_N) } + \sabs{ S_{N}(\zerobf) - V_N(\zerobf) }
\end{align*}
which converges almost surely to zero as $N\rightarrow\infty$ as a result of~\eqref{eq:SNVNunifmlln}.
\end{proof}

% \begin{lemma}\label{lem:ABprob}
% Let $A$ and $B$ be positive random variables.  Then
% \[
% \prob( A + B > \epsilon ) \leq \prob( A > \tfrac{\epsilon}{2}) + \prob( A > \tfrac{\epsilon}{2}).
% \]
% \end{lemma}
% \begin{proof}
% Write 
% \begin{equation}\label{eq:A+B>epsilon}
% \prob( A + B > \epsilon ) = 1 - \prob( A + B \leq \epsilon ).
% \end{equation}
% If $A \leq \tfrac{\epsilon}{2}$ and $B \leq \tfrac{\epsilon}{2}$ then $A + B \leq \epsilon$, so
% \begin{align*}
% \prob( A + B \leq \epsilon ) &\geq \prob( A \leq \tfrac{\epsilon}{2} \;\; \text{and} \;\; B \leq \tfrac{\epsilon}{2} ) \\
% &= 1 - \prob( A > \tfrac{\epsilon}{2} \;\; \text{or} \;\; B > \tfrac{\epsilon}{2} ) \\
% &\geq 1 - \prob( A > \tfrac{\epsilon}{2}) - \prob( B > \tfrac{\epsilon}{2} ).
% \end{align*}
% Subtituting this into~\eqref{eq:A+B>epsilon} gives the required result.
% \end{proof}

We have now shown that $V_N$ is uniquely minimised at $\zerobf$, that $V_N(\zerobf) = \sigma^2$, and that $V_N(\widehat{\lambdabf}_N)$ converges almost surely to $\sigma^2$.  These results are enough to show that $\widehat{\lambdabf}_N$ converges almost surely to zero.  However, this tells us nothing about the rate at which the components of $\widehat{\lambdabf}_N$ approach zero as required by Theorem~\ref{thm:asymp_proof}.  To prove these stronger properties we need some preliminary results about arithmetic progressions, and from the calculus of finite differences.
 
Let $W = \{1,2,\dots, N\}$ and let $K$ be a subset of $W$.  For any integer $h$, let
\begin{equation} \label{eq:S(h,G)def} 
A(h,K) = \big\{ n \mid n + ih \in K \;\forall\; i \in \{0,1,\dots,m\} \big\}
\end{equation}
be the set containing all integers $n$ such that the arithmetic progression
\[
n, \,\, n + h, \,\, n + 2h, \,\, \dots, \,\, n + mh
\]
of length $m+1$ is contained in the subset $K$.  If $K$ is a small subset of $W$ then $A(h,K)$ might be empty. However, the next two lemmas and the following corollary will show that if $K$ is sufficiently large then it always contains at least one arithmetic progression (for all sufficiently small $h$) and therefore $A(h,K)$ is not empty. We do not wish to claim any novelty here.  The study of arithmetic progressions within subsets of $W$ has a considerable history~\cite{Erdos_on_some_sequence_of_integers1936,Szemeredi_setint_no_k_arth1975,Gowers_new_proof2001}.  In particular, Gower's~\cite[Theorem 1.3]{Gowers_new_proof2001} gives a result far stronger than we require here.  Denote by $K \backslash \{r\}$ the set $K$ with the element $r$ removed.

\begin{lemma} \label{lem:S(h,G/r)size}
Let $r \in K$.  For any $h$, removing $r$ from $K$ removes at most $m+1$ arithmetic progressions $n, n+h, \dots n+mh$ of length $m+1$.  That is,
\[
|A(h,K \backslash \{r\})| \geq |A(h,K)| - (m+1).
\]
\end{lemma}
\begin{proof}
The proof follows because there are at most $m+1$ integers, $n$, such that $n+ih = r$ for some $i \in \{0,1,\dots,m\}$.  That is, there are at most $m+1$ arithmetic progressions of type $n, n+h, \dots n+mh$ that contain $r$.
\end{proof}

 \begin{lemma} \label{lem:S(h,K)size}
 $|A(h,K)| \geq N - mh - (N - |K|)(m+1)$.
 \end{lemma}
 \begin{proof}
 Note that $|A(h,W)| = N - mh$.  The proof follows by starting with $A(h,W)$ and applying Lemma~\ref{lem:S(h,G/r)size} precisely $|W|-|K|=N-|K|$ times. That is, $K$ can be constructed by removing $N - |K|$ elements from $W$ and this removes at most $(N - |K|)(m+1)$ arithmetic progressions from $A(h,W)$.
 \end{proof}
 
 \begin{corollary} \label{cor:S(h,K)>0}
 Let $K \subseteq W$ such that $|K| > \frac{2m+1}{2m+2}N$. For all $h$ such that $1\leq h \leq\frac{N}{2m}$ the set $K$ contains at least one arithmetic progression $n, n+h, \dots, n+mh$ of length $m+1$. That is, $|A(h,K)| > 0$.
 \end{corollary}
 \begin{proof}
 By substituting the bounds $|K| > \frac{2m+1}{2m+2}N$ and $h \leq\frac{N}{2m}$ into the inequality from Lemma~\ref{lem:S(h,K)size} we immediately obtain $|A(h,K)| > 0$.
 \end{proof}

The next result we require comes from the calculus of finite differences. For any function $d(n)$ mapping $\reals$ to $\reals$, let 
\[
\Delta_h^1 d(n) = d(n+h) - d(n)
\] 
denote the first difference with interval $h$, and let
% \begin{align*}
% \Delta_h^2 d(n) = \Delta_h d(n+h) - \Delta_h d(n) = d(n+2h) - 2d(n+h) + d(n)
% \end{align*}
% denote the second difference with interval $h$ and similarly let 
\begin{equation}\label{eq:mthdiffformula}
\begin{split}
\Delta_h^r d(n) &= \Delta_h^{r-1} d(n+h) - \Delta_h^{r-1} d(n) \\
&= \sum_{k=0}^{r}\binom{r}{k}(-1)^{r-k}d(n+kh)
\end{split}
\end{equation}
denote the $r$th difference with interval $h$. Since $\sum_{k=0}^{r}\binom{r}{k} = 2^r$ it follows that $\Delta_h^r d(n)$ can be represented by adding and subtracting the 
\[
d(n), \,\, d(n+h), \,\, \dots, \,\, d(n+kh)
\] 
precisely $2^r$ times.

The operator $\Delta_h^r$ has special properties when applied to polynomials. If $d(n) = a_r n^r + \dots + a_0$ is a polynomial of order $r$ then
 \begin{equation} \label{eq:mfinitediffpoly}
 \Delta_h^r d(n) = h^r r! a_r. 
 \end{equation}
So, the $r$th difference of the polynomial is a constant depending on $h$, $r$ and the $r$th coefficient $a_r$~\cite[page 51]{Jordan_Calculus_of_finite_difference_1965}.  We can now continue the proof of strong consistency.  The next lemma is a key result.

\begin{lemma}\label{lem:moran2}
Suppose $\lambdabf_1, \lambdabf_2,\dots$ is a sequence of vectors from $B$ with $V_N(\lambdabf_N) - \sigma^2\rightarrow 0$ as $N\rightarrow\infty$. Then the elements $\lambda_{0,N}, \dots \lambda_{m,N}$ of $\lambdabf_N$ satisfy $N^k\lambda_{k, N}\rightarrow0$ as $N\rightarrow\infty$.
\end{lemma}
\begin{proof}
Define the function
\begin{equation}\label{eq:gz}
g(z) = \expect\fracpart{\Phi_1 + z}^2 - \sigma^2
\end{equation}
which is continuous in $z$. Because of~\eqref{eq:Efracpartmined} and~\eqref{eq:Efracpartphi}, $g(z) \geq 0$ with equality only at $z = 0$ for $z \in [-\nicefrac{1}{2}, \nicefrac{1}{2})$. Now
\[
V_N(\lambdabf_N) - \sigma^2 = \frac{1}{N}\sum_{n=1}^{N} g\left( \fracpart{ \sum_{k=0}^{m}{n^k \lambda_{k,N}} } \right) \rightarrow 0
\]
as $N \rightarrow \infty$. Let
\[
z_N(n) = \lambda_{0,N} + \lambda_{1,N} n + \dots + \lambda_{m,N} n^m
\]
so that 
\[
V_N(\lambdabf_N) - \sigma^2 = \frac{1}{N}\sum_{n=1}^{N} g\left( \fracpart{z_N(n)} \right) \rightarrow 0
\] 
as $N \rightarrow \infty$.
%At this stage we can \emph{see} the proof. Note that, due to the definition of the identifiable region, $B$, the only circular polynomial $\phi$ with values that are all zeros (and hence sends the sum above to zero) is the circular polynomial whose coefficients are all zeros.  Obviously, this observation does not constitute a proof, and in particular gives us little indication as to the order of convergence of the coefficients as required by the theorem. To make the proof 
Choose constants 
\[
c = \frac{2m+1}{2m+2} \qquad \text{and} \qquad 0 < \delta < \frac{1}{2^{2m+1}}
\]
and define the set $K_{N}=\left\{  n\leq N \mid \sabs{\fracpart{z_N(n)}} < \delta \right\}$.  There exists $N_0$ such that for all $N > N_0$ the number of elements in $K_N$ is at least $cN$.  Too see this, suppose that $\sabs{K_N} < cN$, and let $\gamma$ be the minimum value of $g$ over $[-\nicefrac{1}{2},-\delta] \cup [\delta, \nicefrac{1}{2})$. Because $g(0) = 0$ is the unique minimiser of $g$, then $\gamma$ is strictly greater than $0$ and
\begin{align*}
V_N(\lambdabf_N) - \sigma^2 &= \frac{1}{N}\sum_{n=1}^{N} g\left( \fracpart{z_N(n)} \right) \\
&\geq \frac{1}{N}\sum_{n\in K_N} \gamma = (1-c)\gamma,
\end{align*}
violating that $V_N(\lambdabf_N) - \sigma^2$ converges to zero as $N \rightarrow \infty$.  We will assume $N > N_0$ in what follows.

From Corollary \ref{cor:S(h,K)>0} it follows that for all $h$ satisfying $1\leq h \leq\frac{N}{2m}$ the set $A(h,K_N)$ contains at least one element, that is, there exists $n' \in A(h,K_N)$ such that all the elements from the arithmetic progression $n', n'+h, \dots, n' + mh$ are in $K_N$ and therefore 
\[
\sabs{\fracpart{z_N(n')}},\,\, \sabs{\fracpart{z_N(n'+h)}}, \,\, \dots, \,\, \sabs{\fracpart{z_N(n'+mh)}} 
\] 
are all less than $\delta$.  Because the $m$th difference is a linear combination of $2^{m}$ elements (see~\eqref{eq:mthdiffformula}) from 
\[
\fracpart{z_N(n')}, \,\, \fracpart{z_N(n'+h)}, \,\, \dots, \,\, \fracpart{z_N(n'+mh)}
\]
all with magnitude less than $\delta$ we obtain, from Lemma~\ref{lem:fracpartsumanddelta},
\begin{equation}\label{eq:Deltazfracbound}
|\fracpart{\Delta_h^m z_N(n')}| \leq |\Delta_h^m \fracpart{ z_N(n')}| < 2^{m}\delta.
\end{equation}
From \eqref{eq:mfinitediffpoly} it follows that the left hand side is equal to a constant involving $h$, $m$ and $\lambda_{m,N}$ giving the bound
\begin{equation}\label{eq:startiterativearg}
|\fracpart{ h^m m! \lambda_{m,N} }|  = |\fracpart{   \Delta_h^m z_N(n') }| < 2^m\delta
\end{equation}
for all $h$ satisfying $1\leq h \leq\frac{N}{2m}$. Setting $h = 1$ and recalling from~\eqref{eq:identifiability} that $\lambda_{m,N} \in [-\tfrac{0.5}{m!}, \tfrac{0.5}{m!})$, we have
 \[
 |\fracpart{ m! \lambda_{m,N} }| = | m! \lambda_{m,N} |< 2^m\delta.
 \]
Now, because we chose $\delta < \tfrac{1}{2^{2m}}$ it follows that 
\[
| \lambda_{m,N} |< \frac{2^m}{m!}\delta < \frac{1}{m! 2^{m+1}}.
\]
So, when $h = 2$, 
\[
|\fracpart{ 2^m m! \lambda_{m,N} }| = | 2^m m! \lambda_{m,N} |< 2^m\delta
\]
because $2^m m! \lambda_{m,N} \in [-0.5, 0.5)$. Therefore
\[
| \lambda_{m,N} |< \frac{1}{m!}\delta < \frac{1}{m! 2^{2m+1}}.
\]
Now, with $h = 4$, we similarly obtain 
\[
|\fracpart{ 4^m m! \lambda_{m,N} }| = | 4^m m! \lambda_{m,N} |< 2^m\delta
\]
and iterating this process we eventually obtain 
\[
| \lambda_{m,N} | < \frac{2^m}{2^{um} m!}\delta
\]
where $2^u$ is the largest power of 2 less than or equal to $\tfrac{N}{2m}$.  %So $2^{u+1} > \frac{N}{2m}$ and substituting this into the inequality above gives 
%\[
%|\lambda_{m,N}| < \frac{2^m}{ m! \left(\frac{N}{4m}\right)^m}\delta
%\] 
%from which it follows that 
By substituting $2^{u+1} > \frac{N}{2m}$ it follows that
 \begin{equation}\label{eq:enditerativearg}
 N^m|\lambda_{m,N}| < \frac{2^{2m+m}m^m}{m!}\delta
 \end{equation}
for all $N > N_0$.  As $\delta$ is arbitrary, $N^m \lambda_{m,N} \rightarrow 0$ as $N\rightarrow \infty$.

We have now shown that the highest order coefficient $\lambda_{m,N}$ converges as required. The remaining coefficients will be shown to converge by induction.  Assume that $N^k \lambda_{k,N} \rightarrow 0$ for all $k=r+1, r+2, \dots, m$, that is, assume that the $m-r$ highest order coefficients all converge as required. Let
\[
z_{N,r}(n) = \lambda_{0,N} + \lambda_{1,N} n + \dots + \lambda_{r,N} n^r.
\]
Because the $m-r$ highest order coefficients converge we can write $z_N(n) = z_{N,r}(n) + \gamma_N(n)$ where 
\[
\sup_{n\in\{1,\dots,N\}}\abs{\gamma_N(n)} \rightarrow 0 \qquad \text{as $N\rightarrow\infty$}. 
\]
Now the bound from \eqref{eq:Deltazfracbound}, but applied using the $r$th difference, gives
 \begin{equation}\label{eq:zrbound}
\begin{split}
\left|\fracpart{  \Delta_h^r z_N(n')}\right| &= \left|\fracpart{  \Delta_h^r\gamma_N(n') + \Delta_h^r z_r(n') }\right| 
\\ &= |\fracpart{ \epsilon + h^r r! \lambda_{r,N} }| < 2^r\delta,
\end{split}
 \end{equation}
 where
\[
\epsilon = \Delta_h^r \gamma_N(n') \leq 2^r \sup_{n\in\{1,\dots,N\}}\abs{\gamma_N(n)} \rightarrow 0
\] 
as $N\rightarrow\infty$.  Choose $\delta$ and $\epsilon$ such that $2^r\delta < \tfrac{1}{4}$ and $|\epsilon| < \tfrac{1}{4}$.  Then, from \eqref{eq:zrbound} and from Lemma~\ref{lem:fracpartinternalsumlessdelta},
\[
\abs{\fracpart{ h^r r! \lambda_{r,N} }} < 2^r\delta + \abs{\epsilon}
\]
for all $h$ such that $1 \leq h \leq \tfrac{N}{2m}$.  Choosing $2^r\delta + |\epsilon| < 2^{-2r-1}$ and using the same iterative process as for the highest order coefficient $\lambda_{m,N}$  (see~\eqref{eq:startiterativearg}~to~\eqref{eq:enditerativearg}) we find that $N^r \lambda_{r,N} \rightarrow 0$ as $N\rightarrow\infty$.  The proof now follows by induction.
 \end{proof}

% \begin{lemma}\label{sec:proof-strong-cons}
% For any constants $0 \leq c < 1$ and $\delta>0$ there exists an $N_{0}$ such that for all $N > N_0$ the proportion of $\fracpart{z_N(n)}$ with magnitude less than $\delta$ is greater than $c$.  That is, the set
% \[
% K_{N}=\left\{  n\leq N \mid \vert \fracpart{z(n)} \vert < \delta \right\} 
% \]
% has more than $c N$ elements for all $N > N_0$.
% \end{lemma}
% \begin{proof}
% Assume not.  Then for every $N_0$ there exists an $N > N_0$ such that there are more than $(1-c)N$ integers from $1$ to $N$ with $|\fracpart{z(n)}| > \delta$.  Let $\gamma$ be the minimum value of $g$ from~(\ref{eq:gz}) over the interval given by the union $[-\nicefrac{1}{2},-\delta] \cup [\delta, \nicefrac{1}{2})$. Because $g$ is minimised uniquely at $0$ then $\gamma$ is strictly greater than $0$ and the sum
% \[
% \frac{1}{N}\sum_{n=1}^{N} g\left( \fracpart{z(n)} \right) \geq (1-c)\gamma
% \]
% with $(1-c)\gamma$ a positive constant. This violates the fact that $g$ converges to zero as $N \rightarrow \infty$ and the lemma is true by contradiction.
% \end{proof}

 \begin{lemma} \label{lem:fracpartsumanddelta}
 Let $a_1, a_2, \dots, a_r$ be $r$ real numbers such  that $\left|\fracpart{a_n}\right| < \delta$ for all $n = 1,2,\dots,r$.  Then $\left|\fracpart{\sum_{n=1}^r{a_n}}\right| < r\delta.$
 \end{lemma}
 \begin{proof}
 If $\delta > \tfrac{1}{2r}$ the proof is trivial as $\left|\fracpart{\sum_{n=1}^r{a_n}}\right| \leq \tfrac{1}{2}$ for all $a_n \in \reals$.  If $\delta \leq \tfrac{1}{2r}$ then $\fracpart{\sum_{n=1}^r{a_n}} = \sum_{n=1}^r{\fracpart{a_n}}$ and
 \[
 \left\vert\fracpart{\sum_{n=1}^r{a_n}}\right\vert = \left\vert\sum_{n=1}^r{\fracpart{a_n}}\right\vert \leq \sum_{n=1}^r{\left\vert\fracpart{a_n}\right\vert} < r\delta.
 \]
 \end{proof}


\begin{lemma} \label{lem:fracpartinternalsumlessdelta}
Let $\left|\fracpart{a + \epsilon}\right| < \delta$ where $|\epsilon| < \nicefrac{1}{4}$ and $0<\delta<\nicefrac{1}{4}$. Then $\left|\fracpart{a}\right| < \delta + |\epsilon|$.
\end{lemma}
\begin{proof}
By supposition $n - \delta < a + \epsilon < n + \delta$ for some $n \in \ints$.  Since $-\delta - \epsilon > -\tfrac{1}{2}$ and $\delta - \epsilon < \tfrac{1}{2}$, it follows that
\[
n - \tfrac{1}{2} < n - \delta - \epsilon < a < n + \delta - \epsilon < n + \tfrac{1}{2}.
\]
Hence $\fracpart{a} = a - n$ and so
\[
-\delta - \abs{\epsilon} \leq -\delta - \epsilon < \fracpart{a} < \delta - \epsilon \leq \delta + \abs{\epsilon}
\]
and $\abs{\fracpart{a}} \leq \delta + \abs{\epsilon}$.

% The idea behind this proof is to show that under the conditions given $\fracpart{\fracpart{a} + \epsilon}$ does not wrap, i.e. that $\fracpart{a} + \epsilon \in \left[-\nicefrac{1}{2}, \nicefrac{1}{2}\right)$.  To start, assume that $\fracpart{a} + \epsilon \geq \nicefrac{1}{2}$.  Then $\nicefrac{1}{2} \leq \fracpart{a} + \epsilon < \nicefrac{3}{4}$ and
% \[
% -\nicefrac{1}{2} \leq \fracpart{\fracpart{a} + \epsilon} = \fracpart{a + \epsilon} < -\nicefrac{1}{4}
% \] 
% and therefore $|\fracpart{a + \epsilon}| > \nicefrac{1}{4} > \delta$, a contradiction.  Similarly, assume that $\fracpart{a} + \epsilon < -\nicefrac{1}{2}$.  Then $-\nicefrac{1}{2} > \fracpart{a} + \epsilon > -\nicefrac{3}{4}$ and $\nicefrac{1}{2} > \fracpart{a + \epsilon} > \nicefrac{1}{4}$ and therefore $\left|\fracpart{a + \epsilon}\right| > \nicefrac{1}{4} > \delta$, a contradiction.  So, $\fracpart{a} + \epsilon \in \left[-\nicefrac{1}{2}, \nicefrac{1}{2}\right)$ and 
% \[
% |\fracpart{a + \epsilon}| = \left|\fracpart{a} + \epsilon\right| < \delta
% \]
% from which it follows that $\left|\fracpart{a}\right| < \delta + \left|\epsilon\right|$.
\end{proof}

We are now in a position to complete the proof of strong consistency.  As is customary, let $(\Omega, \mathcal F, \prob)$ be the probability space over which the random variables $\{X_i\}$ and $\{\Phi_i\}$ are defined.  Let $A$ be the subset of the sample space $\Omega$ on which  $V_N(\widehat{\lambdabf}_N) - \sigma^2 \rightarrow 0$ as $N\rightarrow\infty$.  Now $\prob\{A\} =1$ as a result of Lemma~\ref{lem:ESNconv}.  Let $A'$ be the subset of $\Omega$ on which $N^k\widehat{\lambda}_{k,N} \rightarrow 0$ for $k=0,\dots,m$ as $N\rightarrow\infty$.  As a result of Lemma~\ref{lem:moran2}, $A \subseteq A'$, and so $\prob\{A'\} \geq \prob\{A\} = 1$.  Strong consistency follows.




%\section{Discussion}
 
\section{Conclusion} \label{sec:conclusion}
 
This paper has considered the estimation of the coefficients of a noisy polynomial phase signal by least squares phase unwrapping (LSU). We have derived conditions under which the polynomial phase estimation problem is identifiable.  Under these conditions, and under some assumptions on the distribution of the noise, the LSU estimator is shown to be strongly consistent. %Simulations are used to show that the LSU has near maximum likelihood performance in the case that the noise is additive white and Gaussian.  
In the second paper in this series~\cite{McKilliam_pps2_2012} we show the LSU estimator to be asymptotically normal and present the results of Monte Carlo simulations that support our asymptotic results.
  


 
%\begin{figure}[p] 
%   	\centering 
%  		\includegraphics[width=\linewidth]{polyfig/polyphase4plot.4} 
%  		\caption{MSE in the third order coefficient $\mu_3$ versus $\var{X_n} = \sigma^2$. The true coefficients are uniformly spread in the identifiable region.} 
%  		\label{plot:polyest_projnorm_mu3} 
% \end{figure} 
 
 
% \begin{figure}[p] 
%  	\centering 
% 		\includegraphics[width=\linewidth]{polyfig/polyphase4plot_unfair.4} 
% 		\caption{MSE in the third order coefficient $\mu_3$ versus $\var{X_n} = \sigma_c^2$. The coefficient have been restricted for Kitchen's estimator and the DPT.} 
% 		\label{plot:polyest_projnorm_mu3_unfair} 
%\end{figure} 
% 
% \begin{figure}[p] 
%  	\centering 
% 		\includegraphics[width=\linewidth]{polyfig/polyphase4plot_fair.4} 
% 		\caption{MSE in the third order coefficient $\mu_3$ versus $\var{X_n} = \sigma_c^2$. The DPT estimator runs at the higher sampling rate $\delta$ so that the volumes $V_{DPT}(\delta) = V_m$.} 
% 		\label{plot:polyest_projnorm_mu3_fair} 
%\end{figure} 
 
 
%\addtolength{\parskip}{-0.2cm} 
%\bibliographystyle{../../bib/IEEEbib} 
\small 
\bibliography{bib} 


\normalsize
\appendix

%\section{Tricks with fractional parts}
%Throughout the paper, and particularly in  have made use of a number of results involving the fractional part function

\subsection{A uniform law of large numbers} \label{app:uniform-law-large}

During the proof of strong consistency we made use of the fact that
\begin{equation}~\label{eq:SNVNunifmlln2}
\sup_{\lambdabf \in B}\sabs{S_N(\lambdabf) - V_N(\lambdabf)} \rightarrow 0
\end{equation}
almost surely as $N\rightarrow\infty$, where $V_N(\lambdabf) = \expect S_N(\lambdabf)$.  We prove this result here.  Put
\[
D_N(\lambdabf) = S_N(\lambdabf) - V_N(\lambdabf).
\]  
Now, for any $\epsilon > 0$,
\[
\sum_{N=1}^\infty \prob \left\{ \sup_{\lambdabf \in B}\abs{ D_N(\lambdabf) } > \epsilon \right\} < \infty
\]
by Lemma~\ref{lem:vn}.  So~\eqref{eq:SNVNunifmlln2} follows from the Borel-Cantelli lemma.   In what follows we use order notation in the standard way, that is, for functions $h$ and $g$, we write $h(N) = O(g(N))$ to mean that there exists a constant $K > 0$ and a finite $N_0$ such that $h(N) \leq K g(N)$ for all $N > N_0$.

\begin{lemma} \label{lem:vn} 
For any $\epsilon > 0$ and $c < 2$,
\[
\prob \left\{ \sup_{\lambdabf \in B}\abs{ D_N(\lambdabf) } > \epsilon \right\} = O(e^{-c \epsilon^2 N}).
\]
 \end{lemma}
\begin{proof}
Consider a rectangular grid of points spaced over the identifiable region $B$.  We use $\lambdabf[\rbf]$, where $\rbf \in \ints^{m+1}$, to denote the grid point 
 %\begin{align*}
\[ 
%\lambdabf[\rbf] = &\left[  \frac{r_0}{N^{b}} - \frac{1}{2}, \; \frac{r_1}{N^{b+1}} - \frac{1}{2}, \right.\\ 
%&\dots, \; \frac{r_k}{N^{b+k}} - \frac{1}{2(k!)}, \; \dots \\
%&\left. \dots, \; \frac{r_m}{N^{b+m}} - \frac{1}{2(m!) }\right]
 \lambdabf[\rbf] = \left[  \frac{r_0}{N^{b}} - \frac{1}{2}, \; \frac{r_1}{N^{b+1}} - \frac{1}{2}, \; \dots, \; \frac{r_m}{m!N^{b+m}} - \frac{1}{2(m!) }\right]
\]
 %\end{align*}
 for some constant $b>0$.  Adjacent grid points are separated by $\tfrac{1}{N^b}$ in the zeroth coordinate, $\tfrac{1}{N^{b+1}}$ in the first coordinate and $\tfrac{1}{k!N^{b+k}}$ in the $k$th coordinate. Let
 \[
 B[\rbf]=\left\{  \xbf\in\reals^{m+1}  ; \frac{r_k}{N^{b+k}}\leq x_k + \frac{1}{2(k!)} < \frac{r_k + 1}{N^{b+k}} \right\}  .
 \]
and let $G$ be the finite set of grid points
\[
G = \left\{ \xbf \in \ints^{m+1} \mid x_k = 0,1,2\dots,N^{b+k}-1  \right\}.
\]
The total number of grid points is $|G| = N^{(m+1)(2b + m)/2}$, and the $B[\rbf]$ partition $B$, that is, $B = \cup_{\rbf \in G}B[\rbf]$.  Now 
\begin{align}
 &\sup_{\lambdabf \in B}\sabs{ D_N(\lambdabf) }  \nonumber \\
&= \sup_{\rbf \in G}\sup_{ \lambdabf \in B[\rbf]}\sabs{ D_N(\lambdabf[\rbf]) + D_N(\lambdabf) - D_N(\lambdabf[\rbf])  } \nonumber \\
 &\leq \sup_{\rbf \in G}\sabs{ D_N(\lambdabf[\rbf])} + \sup_{\rbf \in G}\sup_{\lambdabf\in B[\rbf]}\sabs{ D_N(\lambdabf) - D_N(\lambdabf[\rbf])  }. \label{eq:gridanddensespace}
 \end{align}
From Lemma~\ref{lem:supVjk} it will follow that 
\[
\prob\left\{   \sup_{\rbf \in G }\abs{ D_N(\lambdabf[\rbf])  } > \frac{\epsilon}{2} \right\} = O(e^{-c \epsilon^2 N})
\]
for any $\epsilon > 0$ and $c < 2$.  In Lemma~\ref{lem:supBVn} we show that
\[
\sup_{\rbf \in G}\sup_{\lambdabf\in B[\rbf]}\abs{ D_N(\lambdabf) - D_N(\lambdabf[\rbf])} < 2\frac{m+1}{N^b}.
\]
%that is, $D_N$ does not change much within any of the regions $B[\rbf]$ neighbouring the grid points.  
Combining these results with~\eqref{eq:gridanddensespace}, we obtain
\[
\prob\left(\sup_{\lambdabf \in B }\sabs{ D_N(\lambdabf)} > \frac{\epsilon}{2} +  \frac{2(m+1)}{N^b} \right) = O(e^{-c \epsilon^2 N}),
\]
and for sufficiently large $N$, we have $\epsilon/2 + \frac{2(m+1)}{N^b} < \epsilon$ completing the proof.  It remains to prove Lemmas~\ref{lem:supVjk} and~\ref{lem:supBVn}.
\end{proof}

% We first need the following special case of Hoeffding's inequality~\cite{Hoeffding_inequality_1963}.

% \begin{lemma} \label{lem:zero_mean_indepent_sum_bound}
%   Let $\bar{Z} = (Z_1 + \dots + Z_N)/N$ where $Z_1, \dots, Z_N$ are independent, zero-mean random variables with $|Z_n| \leq \tfrac{1}{4}$ for each $n$.  Then,
% \[
%   \prob\{ \sabs{\bar{Z}} > \epsilon \} \leq 2e^{ - 8 \epsilon^2 N}.
% \]
% \end{lemma}

\begin{lemma}\label{lem:supVjk}
For any $\epsilon > 0$ and $c < 8$,
\[
\prob\cubr{   \sup_{\rbf \in G}\abs{ D_N(\lambdabf[\rbf])  } > \epsilon } = O(e^{-c \epsilon^2 N}).
\]
\end{lemma}
 \begin{proof}
Fix $\lambdabf$ and write 
\[
D_N(\lambdabf) = \bar{Z} = \frac{1}{N}  \sum_{n=1}^{N}Z_{n},
\] 
where
 \[
 Z_{n}=\fracpart{  \Phi_n+\sum_{k = 0}^{m}{\lambda_k n^k} }^{2} - \expect \fracpart{  \Phi_n + \sum_{k = 0}^{m}{\lambda_k n^k} }^{2}
 \]
 are independent with zero mean and $\abs{Z_n} \leq \tfrac{1}{4}$. It follows from Hoeffding's inequality~\cite{Hoeffding_inequality_1963} that,  
\[
\prob\{ \abs{D_N(\lambdabf)} > \epsilon \} \leq 2e^{ - 8 \epsilon^2 N},
\] 
and so,
 \begin{align*}
 \prob\cubr{  \sup_{\rbf\in G}\left\vert D_N(\lambdabf[\rbf]) \right\vert >\epsilon }  &\leq \sum_{\rbf \in G} \prob\scubr{  \left\vert D_N(\lambdabf[\rbf])  \right\vert >\epsilon } \nonumber \\
&= 2 |G| e^{ - 8 \epsilon^2 N} = O(e^{-c \epsilon^2 N}), \label{eq:betaprobconv}
\end{align*}
where $c$ is any real number less than $8$, since $|G| = N^{(m+1)(2b + m)/2}$ is polynomial in $N$.
\end{proof}

Before proving Lemma~\ref{lem:supBVn} we need the following result.

\begin{lemma}\label{lem:boundedsquarefracparts} For real numbers $x$ and $delta$,
\[
\fracpart{x}^2 - |\delta| \leq \fracpart{x + \delta}^2 \leq \fracpart{x}^2 + |\delta|.
\]
\end{lemma}
\begin{proof}
Since $\abs{\delta} \leq \abs{n + \delta}$ for all $\delta \in [-\nicefrac{1}{2}, \nicefrac{1}{2})$ and $n \in \ints$, the result will follow if we can show that it holds when both $x$ and $\delta$ are in $[-\nicefrac{1}{2}, \nicefrac{1}{2})$.  Also, for reasons of symmetry, we need only show that it holds when $\delta \geq 0$.  Now
\[
\fracpart{x + \delta}^2 - x^2 = \begin{cases}
2x\delta + \delta^2, & x \in [-\nicefrac{1}{2}, \nicefrac{1}{2} - \delta) \\
2x(\delta-1) + (\delta-1)^2, & x \in [\nicefrac{1}{2} - \delta, \nicefrac{1}{2})
\end{cases}
\] 
But, when $x \in [-\nicefrac{1}{2}, \nicefrac{1}{2} - \delta)$, 
\[
-1 \leq -1 + \delta \leq 2x + \delta < 1 - \delta \leq 1,
\]
and so
\[
-\delta \leq (-1 + \delta)\delta \leq (2x + \delta)\delta < (1-\delta)\delta \leq \delta.
\]
Also, when $x \in [\nicefrac{1}{2} - \delta, \nicefrac{1}{2})$ we  have $-\delta \leq 2x + \delta - 1 < \delta$, and consequently
\[
-\delta \leq -\delta(1-\delta) \leq (2x + \delta - 1)(1 - \delta) \leq \delta(1 - \delta) \leq \delta.
\]
%and consequenly
%\[
%-\delta \leq (2x + \delta - 1)(\delta - 1) \leq \delta.
%\]

% In some sense this result is `obvious' because  the function $\fracpart{x}^2$ is continuous and piecewise differentiable and the derivative has magnitude less than one whenever it exists.  We will give a rigorous proof of the lemma when $\delta \geq 0$, the proof for $\delta \leq 0$ is similar. Note that $0 \leq \fracpart{x}^2 \leq \tfrac{1}{4}$ for all real numbers $x$. So if $\delta \geq \tfrac{1}{4}$ then the lemma is true because
% \[
% \fracpart{x}^2 - |\delta| \leq 0 \leq \fracpart{x + \delta}^2 \leq \tfrac{1}{4} \leq \fracpart{x}^2 + |\delta|.
% \]
% So we may assume that $\delta < \tfrac{1}{4}$. We consider two cases, firstly when $\fracpart{x} + \delta \in [-\nicefrac{1}{2}, \nicefrac{1}{2})$, and secondly when $\fracpart{x} + \delta \geq \nicefrac{1}{2}$. Assume that $\fracpart{x} + \delta \in [-\nicefrac{1}{2}, \nicefrac{1}{2}]$ and therefore
% \[
% -\tfrac{1}{2} \leq \fracpart{x} \leq \tfrac{1}{2} - \delta
% \]
% and also
% \[
% \fracpart{x + \delta}^2 = \fracpart{\fracpart{x} + \delta}^2 = (\fracpart{x} + \delta)^2 = \fracpart{x}^2 + 2\fracpart{x}\delta + \delta^2.
% \]
% Substituting the bounds on $\fracpart{x}$ above into the right hand side of this equation we obtain
% \[
% \fracpart{x}^2 - \delta(1 - \delta) \leq \fracpart{x + \delta}^2 \leq \fracpart{x}^2 + \delta(1 - \delta).
% \]
% The lemma is true because $\delta(1 - \delta) \leq |\delta|$ since $0 \leq \delta < \tfrac{1}{4}$. Now assume that $\fracpart{x} + \delta \geq \nicefrac{1}{2}$. We have
% \[
% \fracpart{x + \delta} = \fracpart{x} + \delta - 1
% \]
% and therefore
% \[
% \tfrac{1}{2} - \delta \leq \fracpart{x} \leq \tfrac{1}{2}
% \]
% and also 
% \[
% \fracpart{x + \delta}^2 = (\fracpart{x} + \delta - 1)^2 = \fracpart{x}^2 + 2\fracpart{x}(\delta  - 1) + (\delta - 1)^2.
% \]
% Substituting the bounds on $\fracpart{x}$ above into the right hand side of this equation we again obtain
% \[
% \fracpart{x}^2 - \delta(1 - \delta) \leq \fracpart{x + \delta}^2 \leq \fracpart{x}^2 + \delta(1 - \delta)
% \]
% and the lemma is true because $\delta(1 - \delta) \leq |\delta|$ since $0 \leq \delta < \tfrac{1}{4}$.
\end{proof}


 \begin{lemma}\label{lem:supBVn} 
For all positive integers $N$,
\[
\sup_{\rbf\in G}\sup_{ \lambdabf  \in B[\rbf] }\left\vert D_N\left(  \lambdabf \right) - D_N\left(  \lambdabf[\rbf] \right)\right\vert < 2\frac{m+1}{N^b}.
\]
 \end{lemma}
 \begin{proof}
%The proof we give is not just almost surely but \emph{surely} as the convergence will be shown to occur irrespective of the values of the $\Phi_n$. 
Put 
\[
b_n = \Phi_n + \sum_{k = 0}^{m}{\lambda_k n^k} \;\;\; \text{and}  \;\;\; a_n = \Phi_n + \sum_{k = 0}^{m}{\lambda[\rbf]_k n^k},
\]
where $\lambda[\rbf]_k$ denotes the $k$th element of the grid point $\lambdabf[\rbf]$. For $\lambdabf \in B[\rbf]$ we have $b_n = a_n + \delta_n$,
where 
\[
\sabs{\delta_n} \leq \sum_{k=0}^m\frac{n^k}{k! N^{b+k}} \leq \frac{m+1}{N^{b}}.
\]
From Lemma~\ref{lem:boundedsquarefracparts} it follows that 
\[
-|\delta_n| \leq \fracpart{x + b_n}^2 -  \fracpart{x + a_n}^2  \leq |\delta_n|,
\]
and consequently 
\[
|\fracpart{x + b_n}^2 -  \fracpart{x + a_n}^2| \leq \tfrac{m+1}{N^b}
\] 
for all $x \in \reals$. Now
\[
S_N(\lambdabf) - S_N(\lambdabf[\rbf]) =  \frac{1}{N}\sum_{n=1}^{N}\big( \fracpart{\Phi_n + b_n}^2 -  \fracpart{\Phi_n + a_n}^2 \big)
\]
and therefore 
\[
\sabs{S_N(\lambdabf) - S_N(\lambdabf[\rbf])} \leq \frac{m+1}{N^b}
\] 
for all $\lambdabf \in B[\rbf]$.  As this bound is independent of $\Phi_1 \dots \Phi_N$, we have
\[
\sabs{ V_N(\lambdabf) - V_N(\lambdabf[\rbf])} \leq \expect \sabs{ S_N(\lambdabf) - S_N(\lambdabf[\rbf])} \leq \frac{m+1}{N^b}
\]
by Jensen's inequality.  Therefore,
 \begin{align*}
 &\sabs{ D_N(\lambdabf) - D_N(\lambdabf[\rbf]) } \\
&= \sabs{ S_N(\lambdabf) - S_N(\lambdabf[\rbf]) + V_N(\lambdabf) - V_N(\lambdabf[\rbf]) } \\
&\leq \sabs{ S_N(\lambdabf) - S_N(\lambdabf[\rbf]) \vert + \vert V_N(\lambdabf) - V_N(\lambdabf[\rbf]) } \\
&\leq 2\frac{m+1}{N^{b}},
\end{align*}
for all $\lambdabf \in B[\rbf]$.  The lemma follows because this bound is independent of $\rbf$.
%\[
%\sup_{\rbf\in G}\sup_{ \lambdabf  \in B[\rbf] }\sabs{ D_N\left(  \lambdabf \right) - D_N\left(  \lambdabf[\rbf] \right) } \leq 2\frac{m+1}{N^{b}}.
%\]
 \end{proof}



 


\end{document}